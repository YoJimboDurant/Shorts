---
title: Analyzing NHANES data using R
author: Jim Durant
output:
  knitrBootstrap::bootstrap_document:
  theme.chooser: TRUE
  highlight.chooser: TRUE
---
#NHANES Data Analysis Example#

#Introduction
This short tutorial assumes that the user has some prior knowledge of R and Rstudio. If you are not familiar with R, there are number of resources in CDC/ATSDR R user group's website to help you get started. Good resources to keep in mind are:

* [Survey Package Homepage](http://r-survey.r-forge.r-project.org/survey/)
* [adfree](http://www.asdfree.com/)

# NHANES Data Analysis
## Install and load required packages
First we will install and load the packages needed to perform this script.
This snippet checks to see if you have the packages needed installed, and
installs any missing packages:

```{r installpacks, eval = TRUE}
pack_list <- c("survey", "downloader", "foreign", "XML", "dplyr", "magrittr",
"ggplot2", "readr", "stringr")

install.packages(pack_list[!pack_list %in% installed.packages()])
```
It is normally not necissary to re-install packages in a given R installation.


Next, we need to load the packages into memory.
```{r libs, eval=FALSE}
library(downloader)
library(foreign)
library(survey)
library(XML)
library(ggplot2)
library(readr)
library(stringr)
library(magrittr)
library(dplyr)
```

## Basic Workflow

### Download raw survey data from [CDC Website](https://wwwn.cdc.gov/Nchs/Nhanes/Search/Nhanes_continuous.aspx). Main pages for continious NHANES data are:

* [Demographics Variables and Sample Weights](https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Demographics) (almost always needed for any analysis)
* [Dietary Interview](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=Dietary)
* [Labratory Data](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=Laboratory)
* [Questionaire Data](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=Questionnaire)

These files are stored as SAS XPORT files on CDC's internet site. R can be used to rapidly expedite the downloading of these files.


<div class="alert alert-dismissible alert-warning">
  <button type="button" class="close" data-dismiss="alert">&times;</button>
  <h4>Tip!</h4>
  <p> Using R to download files will help make your analysis easier to reproduce.</p>
</div>


#### Identify Files

```{r id files}
demoDataFiles <- read_lines("https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Demographics") %>%
  XML::htmlParse(.) %>%
  XML::readHTMLTable(.)

knitr::kable(demoDataFiles[[2]])
```

We would like to select all demographics surveys in this dataset and download to our local computer.

First, make a directory:
```{r createDir}
dataDir <- paste0("./data/nhanes/", format(Sys.Date(), "%Y%m%d"))
dir.create(dataDir, showWarnings = FALSE, recursive = TRUE)
```

<div class="alert alert-dismissible alert-warning">
  <button type="button" class="close" data-dismiss="alert">&times;</button>
  <h4>Tip!</h4>
  <p> Why did I create a system date? If you noticed that in the XPT files, there are revision dates. If the data are updated and previous versions of the data are unavailable, then you might not be able to guarentee that your results are reproducible because the data may have changed. Best practice is to maintain an archive of original data, and create a copy of working data that is further manipulated.
</p>
</div>

```{r idFiles}
idFiles <- grep("XPT", demoDataFiles[[2]][,4]) %>%
  demoDataFiles[[2]][.,] %T>%
  knitr::kable(.)
```

#### Download the files

The SAS XPT Files
```{r downloadFiles}
serverFiles <- gsub( " .*$", "", idFile$`Data File`) %>%
  paste0(., ".XPT") %>%
  paste("https://wwwn.cdc.gov/Nchs/Nhanes", idFiles$Years, .,  sep="/")


if(Sys.info()['sysname'] !="Windows"){
  sapply(serverFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), method="curl", extra="-k"))
} else {
  sapply(serverFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), mode="wb"))
}
```

and the Documentation (we not only read these files, but since they are HTML we can extract the code books from them using R):

```{r Download Docs}
docFiles <- gsub("XPT$", "htm", serverFiles)

if(Sys.info()['sysname'] !="Windows"){
sapply(docFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), method="curl", extra="-k"))
}else{
sapply(docFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), mode= "wb"))
}
```


### Import the XPT Files and Decode using Data
Next, we will make a list of files that we have downloaded. Then we will use `lapply` to create a  loop that will read these files using `read.xport` into a list of data frames that we named demoData.

```{r foreign Import}
localFiles <- list.files(dataDir, pattern = "[.]XPT", full.names = TRUE)
demoData <- lapply(localFiles, function(x) foreign::read.xport(x))
names(demoData) <- localFiles
```

I _HATE_ recoding variables in R.  So I create 2 loops. The first loop goes through each survey set. The second loop extracts the tables that have factor codes (NHANES documentation so far has consistently indicated continious ranges with number values separated by "to"). The function then refactors the variable with the appropriate code book names.

```{r recode,  bootstrap.show.output=FALSE}
xDocs <- gsub("XPT$", "htm", localFiles)

factorTables <- sapply(seq_along(xDocs), function(xDoci){


  x <- XML::htmlParse(xDocs[[xDoci]]) %>%
    XML::readHTMLTable(.)

  x <- x[sapply(x, function(xi) "Code or Value" %in% names(xi))]

  factorVars <- sapply(seq_along(x), function(i) {
    !grepl("to", x[[i]]$`Code or Value`[[1]])
    })

  factorVars <- factorVars[!sapply(factorVars, is.null)]

FactTables <- sapply(seq_along(factorVars), function(i){
    if(!is.null(factorVars[[i]])){
    if(factorVars[[i]] == TRUE){
      #print(i)
      xTable <- x[[i]]
      #browser()
      is.na(xTable$`Code or Value`) <- xTable$`Code or Value` == "."
      factorLevels <- xTable$`Value Description`[!is.na(xTable$`Code or Value`)]
      factorLevels <- factorLevels[as.numeric(as.character(xTable$Count[!is.na(xTable$`Code or Value`)])) > 0 ]
      demoData[[xDoci]][[i+1]] <- factor(demoData[[xDoci]][[i+1]], labels = factorLevels)
      table(demoData[[xDoci]][[i+1]])

    }
    }

  })
return(FactTables)
})

```

I need to figure out how to add table names. The are stored as vector names in the data frames as their abbreviated names. Most folks get used to these after a while and know what they mean. I am not "most folks."

```{r get names}
VariableNamesList <-

  lapply(seq_along(xDocs), function(xDoci){

    xD = xDocs[[xDoci]]
    xDatNames <- demoData[[xDoci]] %>% names(.)

   x <- XML::htmlParse(xD) %>%
     readHTMLList(.)
   xi <- which(sapply(x, length) == length(xDatNames))

   #check all vars in name
   checkVars <-
     sapply(seq_along(xDatNames),
          function(vi) grepl(xDatNames[[vi]],
                             paste(x[[xi]], collapse = " "))) %>%
     all(.) &
   length(xDatNames) == length(x[[xi]])
   names(demoData[[xDoci]]) <- x[[xi]]
   return(data.frame(abbr = xDatNames, full = x[[xi]]))
  })


```





```{r prep}
factorTables <- lapply(seq_along(factorTables), function(i){
  factorTables[[i]][!sapply(factorTables[[i]], is.null)]
})

names(factorTables) <- basename(localFiles)

pander::pander(factorTables[[2]][[4]])
```


<div align="center" class="embed-responsive embed-responsive-16by9">
<video autoplay loop class="embed-responsive-item">
<source src=https://www.youtube.com/watch?v=gsX4I133Zmc type=video/mp4>
</video>
</div>



<div class="panel panel-danger">
  <div class="panel-heading">
  <h3 class="panel-title">Panel danger</h3>
  </div>
  <div class="panel-body">
    Panel content
  </div>
</div>

<div class="panel panel-info">
  <div class="panel-heading">
  <h3 class="panel-title">Panel info</h3>
  </div>
  <div class="panel-body">
    Panel content
  </div>
</div>

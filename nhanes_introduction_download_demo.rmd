---
title: Analyzing NHANES data using R - Introduction and Downloading Demographic Data
author: Jim Durant
output:
  knitrBootstrap::bootstrap_document:
  theme.chooser: TRUE
  highlight.chooser: TRUE
---
#NHANES Data Analysis: Introduction and Downloading Demographic Data#

#Introduction
This short tutorial assumes that the user has some prior knowledge of R and Rstudio. If you are not familiar with R, there are number of resources in CDC/ATSDR R user group's website to help you get started. Good resources to keep in mind are:

* [Survey Package Homepage](http://r-survey.r-forge.r-project.org/survey/)
* [adfree](http://www.asdfree.com/)

# NHANES Data Analysis
## Install and load required packages
First we will install and load the packages needed to perform this script.
This [snippet](otherpage.html#header1) checks to see if you have the packages needed installed, and
installs any missing packages:

```{r installpacks, eval = TRUE}
pack_list <- c("survey", "downloader", "foreign", "XML", "dplyr", "magrittr",
"ggplot2", "readr", "stringr")

if(length(pack_list[!pack_list %in% installed.packages()])>0)
  install.packages(pack_list[!pack_list %in% installed.packages()])
```
It is normally not necissary to re-install packages in a given R installation.


Next, we need to load the packages into memory.
```{r libs, message= FALSE}
library(downloader)
library(foreign)
library(survey)
library(XML)
library(ggplot2)
library(readr)
library(stringr)
library(magrittr)
library(dplyr)
```

## Basic Workflow

### Download raw survey data from [CDC Website](https://wwwn.cdc.gov/Nchs/Nhanes/Search/Nhanes_continuous.aspx). Main pages for continious NHANES data are:

* [Demographics Variables and Sample Weights](https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Demographics) (almost always needed for any analysis)
* [Dietary Interview](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=Dietary)
* [Labratory Data](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=Laboratory) - [Example with Arsenic from NHANES, march 2018](https://www.cdc.gov/exposurereport/pdf/FourthReport_UpdatedTables_Volume1_Mar2018.pdf)
* [Questionaire Data](https://wwwn.cdc.gov/Nchs/Nhanes/Search/DataPage.aspx?Component=Questionnaire)

These files are stored as SAS XPORT files on CDC's internet site. R can be used to rapidly expedite the downloading of these files.


<div class="alert alert-dismissible alert-warning">
  <button type="button" class="close" data-dismiss="alert">&times;</button>
  <h4>Tip!</h4>
  <p> Using R to download files will help make your analysis easier to reproduce.</p>
</div>


#### Identify Files

Demographic Files:
```{r id files}
demoDataFiles <- read_lines("https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Demographics") %>%
  XML::htmlParse(.) %>%
  XML::readHTMLTable(.) %$%
  GridView1

knitr::kable(demoDataFiles)
```

Arsenic Laboratory Files:
```{r labfiles}
labDataFiles <- read_lines("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory") %>%
  XML::htmlParse(.) %>%
  XML::readHTMLTable(.) %$%
  GridView1%>%
  filter(grepl("Arsenic", `Data File Name`))

knitr::kable(labDataFiles$`Data File`)
```


We would like to select all demographics surveys and labratory data in this dataset and download to our local computer.

First, make a directory:
```{r createDir}
dataDir <- paste0("./data/nhanes/", format(Sys.Date(), "%Y%m%d"))
dir.create(dataDir, showWarnings = FALSE, recursive = TRUE)
```

<div class="alert alert-dismissible alert-warning">
  <button type="button" class="close" data-dismiss="alert">&times;</button>
  <h4>Tip!</h4>
  <p> Why did I create a system date? If you noticed that in the XPT files, there are revision dates. If the data are updated and previous versions of the data are unavailable, then you might not be able to guarentee that your results are reproducible because the data may have changed. Best practice is to maintain an archive of original data, and create a copy of working data that is further manipulated.
</p>
</div>

Then we identify our files:

```{r idFiles}
idFiles <- suppressWarnings(
    bind_rows(
    filter(demoDataFiles, grepl("XPT", `Data File`)),
    filter(labDataFiles, grepl("XPT", `Data File`))
  )
)


  
  knitr::kable(idFiles)
```

#### Download the demographics files

Download the SAS XPT Files
```{r downloadFiles}
serverFiles <- gsub( " .*$", "", idFiles[,"Data File"]) %>%
  paste0(., ".XPT") %>%
  paste("https://wwwn.cdc.gov/Nchs/Nhanes", idFiles$Year, .,  sep="/")

destfile = sapply(serverFiles, function(x) paste(dataDir, basename(x),  sep="/"))

serverFiles <- serverFiles[!file.exists(destfile)]
destfile <- destfile[!file.exists(destfile)]


if(Sys.info()['sysname'] !="Windows"){
  sapply(serverFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), method="curl", extra="-k"))
} else {
  sapply(serverFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), mode="wb"))
}

# download urine cratine for 2015-2016 sample

download.file("https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/ALB_CR_I.XPT", destfile = paste(dataDir, "ALB_CR_I.XPT"))
```

and the Documentation (we not only read these files, but since they are HTML we can extract the code books from them using R):

```{r Download Docs}
docFiles <- gsub("XPT$", "htm", serverFiles)

if(Sys.info()['sysname'] !="Windows"){
sapply(docFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), method="curl", extra="-k"))
}else{
sapply(docFiles, function(x) download.file(x, destfile = paste(dataDir, basename(x),  sep="/"), mode= "wb"))
}
```


# 

### Import the XPT Files and Decode using Data
Next, we will make a list of files that we have downloaded. Then we will use `lapply` to create a  loop that will read these files using `read.xport` into a list of data frames that we named demoData.

```{r foreign Import}
localFiles <- list.files(dataDir, pattern = "DEMO_?[A-Z]?[.]XPT", full.names = TRUE)
demoData <- lapply(localFiles, function(x) foreign::read.xport(x))
names(demoData) <- basename(localFiles)

uasFiles <- list.files(dataDir, pattern = "UAS_?[A-Z]?[.]XPT", full.names = TRUE)
uasData <- lapply(uasFiles, function(x) foreign::read.xport(x))
names(uasData) <- basename(uasFiles)

uassFiles <- list.files(dataDir, pattern = "UASS_?[A-Z]?[.]XPT", full.names = TRUE)
uassData <- lapply(uassFiles, function(x) foreign::read.xport(x))
names(uassData) <- basename(uassFiles)

utasFiles <- list.files(dataDir, pattern = "UTAS_?[A-Z]?[.]XPT", full.names = TRUE)

utasData <- lapply(utasFiles, function(x) foreign::read.xport(x))
names(utasData) <- basename(utasFiles)



```

I _HATE_ recoding variables in R.  So I create 2 loops. The first loop goes through each survey set. The second loop extracts the tables that have factor codes (NHANES documentation so far has consistently indicated continious ranges with number values separated by "to"). The function then refactors the variable with the appropriate code book names.

```{r recode,  bootstrap.show.output=FALSE}
xDocs <- gsub("XPT$", "htm", localFiles)

factorTables <- sapply(seq_along(xDocs), function(xDoci){


  x <- XML::htmlParse(xDocs[[xDoci]]) %>%
    XML::readHTMLTable(.)

  x <- x[sapply(x, function(xi) "Code or Value" %in% names(xi))]

  factorVars <- sapply(seq_along(x), function(i) {
    !grepl("to", x[[i]]$`Code or Value`[[1]])
    })

  factorVars <- factorVars[!sapply(factorVars, is.null)]

FactTables <- sapply(seq_along(factorVars), function(i){
    if(!is.null(factorVars[[i]])){
    if(factorVars[[i]] == TRUE){
      #print(i)
      xTable <- x[[i]]
      #browser()
      is.na(xTable$`Code or Value`) <- xTable$`Code or Value` == "."
      factorLevels <- xTable$`Value Description`[!is.na(xTable$`Code or Value`)]
      factorLevels <- factorLevels[as.numeric(as.character(xTable$Count[!is.na(xTable$`Code or Value`)])) > 0 ]
      demoData[[xDoci]][[i+1]] <- factor(demoData[[xDoci]][[i+1]], labels = factorLevels)
      table(demoData[[xDoci]][[i+1]])

    }
    }

  })
return(FactTables)
})

```

I need to figure out how to add table names. The are stored as vector names in the data frames as their abbreviated names. Most folks get used to these after a while and know what they mean. I am not "most folks."

```{r get names}
VariableNamesList <-

  lapply(seq_along(xDocs), function(xDoci){

    xD = xDocs[[xDoci]]
    xDatNames <- demoData[[xDoci]] %>% names(.)

   x <- XML::htmlParse(xD) %>%
     readHTMLList(.)
   xi <- which(sapply(x, length) == length(xDatNames))

   #check all vars in name
   checkVars <-
     sapply(seq_along(xDatNames),
          function(vi) grepl(xDatNames[[vi]],
                             paste(x[[xi]], collapse = " "))) %>%
     all(.) &
   length(xDatNames) == length(x[[xi]])
   names(demoData[[xDoci]]) <- x[[xi]]
   return(data.frame(abbr = xDatNames, full = x[[xi]]))
  })


```





```{r prep}
factorTables <- lapply(seq_along(factorTables), function(i){
  factorTables[[i]][!sapply(factorTables[[i]], is.null)]
})

names(factorTables) <- basename(localFiles)

pander::pander(factorTables[[8]][[3]])

pander::pander(factorTables[[8]][[4]])

```
this agrees with documentation files.


# Save Data Files

```{r save}
write_rds(factorTables, path = "./data/nhanes/factorTables.rds")
write_rds(demoData, path = "./data/nhanes/demo.rds")
write_rds(utasData, path = "./data/nhanes/utas.rds")

```
